import pandas as pd
import numpy as np
from sklearn.model_selection impor train_test_split
import time as tm

#Metricas
from sklearn.metrics import ExtraTreesClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from sklearn import model_selection

#Classifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier

import warnings
warnings.filterwarnings('ignore')

[] def classifier_metrics():
    def metrics (model):
        start_time = tm.time()
        model.fit(X_train, y_train)
        TIME=tm.time()-start_time
        print("Training Time: {0:.4f} [seconds]".format(TIME))

        start_time = tm.time()
        y_pred=model.predict(X_test)
        TIME=tm.time()-start_time
        print("Prediction Time: {0:.4f} [seconds]".format(TIME))

        try:
            y_prob=model.predict_proba(X_test)
            log_metric=log_loss(y_test, y_prob)
        except:
            y_prob="Not probabilistic"
            log_metric=0
        else:
            y_pred=model.predict(X_test)
        
        acc_score=accuracy_score(y_test,y_pred)
        precision_s=precision_score(y_test,y_pred,average='macro')
        recall_s=recall_score(y_test,y_pred,average='macro')
        f1_s=f1_score(y_test,y_pred,average='macro')
        print('accuracy_score: {0:.4f} [seconds]'.format(acc_score))
        print('precision_score: {0:.4f} [seconds]'.format(precision_s))
        print('recall_score: {0:.4f} [seconds]'.format(recall_s))
        print('f1_score: {0:.4f} [seconds]'.format(f1_s))
    
    for name in classifiers:
        print(str(name))
        metrics(name)
        print()
        print("-----------------------------------------------------------------------\n")

#Carge de datos
path_data='.../Databases'
path_sampledata=path_data+"/12DB_6FP.cvs"
Data=pd.read_csv(path_sampledata)
Data

# Ajuste de datos de las variables categóricas
name_clases={0:"DB",1:"SS",2:"SW",3:"A",4:"I",5:"B"}
Data['FlowPattern']=Data['FlowPattern'].replace(name_clases)
Data

#Obtener las features
features=Data['FlowPattern']
labels.head()

#Separación de la data, con un 20% para testing y 80% para entrenamiento
X_train, X_test, y_train, y_test= train_test_split(features,labels, test_size=0.20, random_state=1, stratify=labels)

#Verificación de la cantidad de datos para entrenamiento y para testing
print("y_train labels unique: ",np.unique(y_train, return_counts=True))
print("y_test labels unique: ",np.unique(y_test, return_counts=True))

classes=["DB","SS","SW","A","I","B"]

#Seleccionar los classifiers
classifier=[
GradientBoostingClassifier(max_depth=7,max_features='sqrt',n_estimators=150,random_state=0),
ExtraTreesClassifier(random_state=0),
BaggingClassifier(max_features=0.9,max_samples=0.9,n_estimators=200,random_state=0),
RandomForestClassifier(n_estimators=50, random_state=0),
DecisionTreeClassifier(criterion='entropy',random_state=0)
]

names=['GB','ET','B','RF','DT']

#Deploy aggregate metrics
classifier_metrics()

##Cross Validation
X_train_=np.concatenate([X_train,X_test],axis=0)
y_train_=np.concatenate([y_train,y_test],axis=0)
print('Data shape: ',X_train_.shape)
print('Data shape: ',y_train_.shape)

models=[]
models.append(('GB',GradientBoostingClassifier(max_depth=7,max_features='sqrt',n_estimators=150,random_state=0)))
models.append(('ET',ExtraTreesClassifier(random_state=0)))
models.append(('B',BaggingClassifier(max_features='sqrt',max_samples=0.9,n_estimators=200,random_state=0)))
models.append(('RF',RandomForestClassifier(n_estimators=50,random_state=0)))
models.append(('DT',DecisionTreeClassifier(criterion='entropy',random_state=0)))

results=[]
names=[]
for name,model in models:
    kfold=model_selection.KFold(n_splits=10, random_state=64,shuffle=True)
    start_time=tm.time()
    cv_results=model_selection.cross_val_score(model,X_train_,y_train_,cv=kfold,scoring='accuracy',n_jobs=-1)
    TIME=tm.time()-start_time
    print("Time: {0:4f}[seconds]".format(TIME))
    results.append(cv_results)
    names.append(name)
    msg="%s: %f (%f)\n"%(name, cv_results.mean(),cv_results.std())
    print(msg)

##Guardado de los modelos
import pickle

    #Entrenar los modelos
modelGB=GradientBoostingClassifier(max_depth=7,max_features='sqrt',n_estimators=150,random_state=0)
modelET=ExtraTreesClassifier(random_state=0)
modelRF=RandomForestClassifier(n_estimators=50,random_state=0)
modelB=BaggingClassifier(max_features=0.9,max_samples=0.9,n_estimators=200,random_state=0)
modelDT=DecisionTreeClassifier(criterion='entropy',random_state=0)

modelGB.fit(X_train,y_train)
modelET.fit(X_train,y_train)
modelRF.fit(X_train,y_train)
modelB.fit(X_train,y_train)
modelDT.fit(X_train,y_train)

#Guardar modelos en archivo
with open('modelGB.pkl','wb')as archivo:
    pickle.dump(modelGB, archivo)

with open('modelET.pkl','wb')as archivo:
    pickle.dump(modelET, archivo)

with open('modelRF.pkl','wb')as archivo:
    pickle.dump(modelRF, archivo)

with open('modelB.pkl','wb')as archivo:
    pickle.dump(modelB, archivo)

with open('modelDT.pkl','wb')as archivo:
    pickle.dump(modelDT, archivo)

